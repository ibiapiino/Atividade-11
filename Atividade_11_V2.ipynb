{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oCWjO5L39mxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90556f98-2630-4921-aefc-4f880286109a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/328.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.9/328.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers sentence-transformers torch pypdf accelerate\n",
        "\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n",
        "from pypdf import PdfReader\n",
        "import logging\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Silencia logs desnecessários\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "def carregar_pdf_zao(caminho_arquivo):\n",
        "    print(f\"Lendo PDF: {caminho_arquivo}...\")\n",
        "    try:\n",
        "        reader = PdfReader(caminho_arquivo)\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "    texto_total = \"\"\n",
        "    for page in reader.pages:\n",
        "        texto_total += page.extract_text()\n",
        "\n",
        "    texto_limpo = texto_total.replace('\\n', ' ').replace('  ', ' ')\n",
        "\n",
        "    tamanho_pedaco = 1200\n",
        "    overlap = 200\n",
        "\n",
        "    pedacos = []\n",
        "    for i in range(0, len(texto_limpo), tamanho_pedaco - overlap):\n",
        "        pedaco = texto_limpo[i:i+tamanho_pedaco]\n",
        "        if len(pedaco) > 50:\n",
        "            pedacos.append(pedaco)\n",
        "\n",
        "    print(f\"Base pronta! {len(pedacos)} trechos longos gerados.\")\n",
        "    return pedacos\n",
        "\n",
        "from google.colab import drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "meu_caminho = '/content/drive/MyDrive/6 semestre/IA/Atividade 11/baseConhecimentoAtividade11.pdf'\n",
        "base_de_conhecimento = carregar_pdf_zao(meu_caminho)\n",
        "\n",
        "print(\"Baixando TinyLlama... (Pode demorar uns 2 minutinhos)\")\n",
        "\n",
        "modelo_busca = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings_base = modelo_busca.encode(base_de_conhecimento, convert_to_tensor=True)\n",
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                device_map=\"auto\")\n",
        "\n",
        "def sistema_rag_pro(pergunta):\n",
        "    embedding_pergunta = modelo_busca.encode(pergunta, convert_to_tensor=True)\n",
        "    resultados = util.semantic_search(embedding_pergunta, embeddings_base, top_k=2)\n",
        "\n",
        "    contexto = \"\"\n",
        "    for hit in resultados[0]:\n",
        "        idx = hit['corpus_id']\n",
        "        contexto += base_de_conhecimento[idx] + \"\\n---\\n\"\n",
        "\n",
        "    print(f\"\\n--- Pergunta: {pergunta} ---\")\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\"\"Você é um especialista em Compliance e Ética da Avengers.\n",
        "            DIRETRIZES OBRIGATÓRIAS:\n",
        "            1. IDIOMA: Responda APENAS usando Português do Brasil.\n",
        "            2. ATENÇÃO ÀS PROIBIÇÕES: Verifique se o texto menciona \"Proibido\", \"Vedado\", \"Não aceitamos\" ou \"Situações Proibidas\" antes da informação.\n",
        "            3. Não forneça respostas ambiguas, quando não tiver o contexto necessario responda que se deve falar com o squad de compliance.\n",
        "            4. tudo o que for \"estritamente profissional\" nao deve nunca ser usado para algo que não seja o trabalho ou envolva o ambito profissional.\n",
        "            5. Seja direto e baseie-se APENAS no contexto abaixo.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Contexto:\\n{contexto}\\n\\nPergunta: {pergunta}\"\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    outputs = pipe(prompt, max_new_tokens=300, do_sample=True, temperature=0.1, top_k=50, top_p=0.95)\n",
        "\n",
        "    resposta_limpa = outputs[0][\"generated_text\"].split(\"<|assistant|>\")[-1].strip()\n",
        "    return resposta_limpa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA7xbzum9qjE",
        "outputId": "372d3bf1-cd0b-496f-83e6-4bed7b53de33"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lendo PDF: /content/drive/MyDrive/6 semestre/IA/Atividade 11/baseConhecimentoAtividade11.pdf...\n",
            "Base pronta! 9 trechos longos gerados.\n",
            "Baixando TinyLlama... (Pode demorar uns 2 minutinhos)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"R1:\", sistema_rag_pro(\"Posso aceitar brindes de parceiros?\"), '\\n\\n')\n",
        "print(\"R2:\", sistema_rag_pro(\"Posso postar numa rede social novidades sobre um projeto que esta em desenvolvimento?\"), '\\n\\n')\n",
        "print(\"R3:\", sistema_rag_pro(\"Posso usar o notebook do trabalho para uso pessoal?\"), '\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00FyBbm79s_7",
        "outputId": "01f4ee4d-49cc-4921-9850-689269df25be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Pergunta: Posso aceitar brindes de parceiros? ---\n",
            "R1: Sure, you can accept gifts or benefits from partners as long as they are strictly professional and do not involve your work or the professional sphere. This includes things like parcel deliveries, discounts, and hospitality. However, you should base your decision on the context and ensure that it is aligned with the Avengers' values and principles. \n",
            "\n",
            "\n",
            "\n",
            "--- Pergunta: Posso postar numa rede social novidades sobre um projeto que esta em desenvolvimento? ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Larissa Pereira Ibiapino \\nCiencia da computacao 6an\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7nNivRmLjGQ",
        "outputId": "dc29655d-90ec-44c6-e353-b19692575d89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Larissa Pereira Ibiapino \n",
            "Ciencia da computacao 6an\n"
          ]
        }
      ]
    }
  ]
}